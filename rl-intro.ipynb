{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in a nutshell\n",
    "\n",
    "(Apprendimento per Rinforzo in poche parole)\n",
    "\n",
    "![“agent-environment-loop”](./assets/AE_loop.png)\n",
    "\n",
    "Copyright © 2024 [Farama Foundation](https://farama.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff Walking: living on the edge\n",
    "\n",
    "- Useremo [`Cliff walking`](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) come esempio:\n",
    "\n",
    "  ![](./assets/cliffw_reset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente\n",
    "\n",
    "Cosa (non) conosce il nostro \"agente\":\n",
    "\n",
    "1. posizione\n",
    "   - **non** sono coordinate!\n",
    "2. possibili azioni (0, 1, 2, 3)\n",
    "   - **non** sa a cosa corrispondono!\n",
    "3. **non** conosce la mappa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente\n",
    "\n",
    "- ogni azione sposta l'agente\n",
    "- precipizio riporta al via\n",
    "- **Premio** (reward):\n",
    "  - -1 ogni movimento\n",
    "  - -100 precipizio\n",
    "  - +100 obiettivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Farama Gymnasium](https://gymnasium.farama.org): una palestra per AI\n",
    "\n",
    "- interfaccia Python e librerie per interagire con ambienti simulati\n",
    "- astrazione di un ambiente per la simulazione\n",
    "  - osservazioni\n",
    "  - azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfaccia\n",
    "\n",
    "Interfaccia costruita intorno all'ambiente di simulazione ([`Env`](https://gymnasium.farama.org/api/env/#gymnasium.Env))\n",
    "\n",
    "- [`make()`](https://gymnasium.farama.org/api/registry/#gymnasium.make): crea un ambiente di simulazione\n",
    "- [`Env.reset()`](https://gymnasium.farama.org/api/env/#gymnasium.Env.reset): inizializza l'ambiente\n",
    "- [`Env.step()`](https://gymnasium.farama.org/api/env/#gymnasium.Env.step); esegue un'azione\n",
    "- [`Env.render()`](https://gymnasium.farama.org/api/env/#gymnasium.Env.render): genera una rappresentazione della situazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interazione con la simulazione\n",
    "\n",
    "- **episodio**: una simulazione\n",
    "  - inizializzazione\n",
    "  - ciclo azione-osservazione\n",
    "- conclusione\n",
    "  - situazione conclusiva (e.g. crash)\n",
    "  - dopo in certo numero di azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = choose_action(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per approfondimenti consultare la [documentazione](https://gymnasium.farama.org/introduction/basic_usage/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "- obiettivo: imparare una *strategia*\n",
    "  - *cosa fare in una situazione*\n",
    "  - useremo \"situazione\"/\"contesto\"/\"stato\" come sinonimi\n",
    "- tabella $Stato\\rightarrow Azione$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table\n",
    "\n",
    "- massimizzare il *risultato*\n",
    "- tabella $Stato \\times Azione \\rightarrow Valore$\n",
    "- il valore è il risultato *stimato* del fare l'azione nel contesto\n",
    "  - somma delle future ricompense\n",
    "- una possibile strategia:\n",
    "  - scegli l'azione che massimizza il risultato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imparare la Q-table\n",
    "\n",
    "- immaginiamo di avere una tabella $Q$\n",
    "- la miglioriamo con l'*esperienza*:\n",
    "  - proviamo $a$ in $s$, finiamo in $s'$ con una ricompensa di $r$\n",
    "  - usiamo $r$ per migliorare $Q(s,a)$:\n",
    "    $$Q(s,a) \\leftarrow Q(s,a) + \\delta_r$$\n",
    "- continuiamo finché siamo soddisfatti della tabella\n",
    "  - otteniamo \"buoni\" risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "\n",
    "> proviamo $a$ in $s$, finiamo in $s'$ con una ricompensa di $r$\n",
    "\n",
    "$$Q(s,a) \\leftarrow \\underbrace{Q(s,a)}_{\\text{valore corrente}} + \\underbrace{\\alpha}_{\\text{tasso di apprendimento}} \\cdot \\bigg( \\underbrace{\\underbrace{r}_{\\text{ricompensa}} + \\underbrace{\\gamma}_{\\text{fattore di sconto}} \\cdot \\underbrace{\\max_{a'}Q(s', a')}_{\\text{valore futuro stimato}}}_{\\text{valore appreso}} - \\underbrace{Q(s,a)}_{\\text{valore corrente}}\\bigg)$$\n",
    "\n",
    "- **tasso di apprendimento** ($\\alpha$): quanto influisce la nuova informazione\n",
    "- **fattore di sconto** ($\\gamma$): l'importanza del valore futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esplorazione o riutilizzo?\n",
    "\n",
    "- come scegliere la prossima azione?\n",
    "  - se l'agente non prova, non impara\n",
    "  - se prova a caso, rischia di esplorare strategie poco utili\n",
    "- bilanciamento tra esplorazione e sfruttamento di quello già imparato\n",
    "  - sceglie un'azione a caso con una certa probabilità\n",
    "  - altrimenti sceglie quella col valore migliore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning per Cliff Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Sequence, SupportsFloat\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import ActType, ObsType, RenderFrame\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import ArtistAnimation, TimedAnimation\n",
    "\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Optional[Callable[[ObsType], ActType]]=None) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "    \"\"\"Run an episode of the environment using the provided agent (default to random), returning the last `step` method output\"\"\"\n",
    "    observation, info = env.reset()\n",
    "    if agent == None:\n",
    "        agent = lambda o: env.action_space.sample()\n",
    "    while True:\n",
    "        action = agent(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            return observation, reward, terminated, truncated, info\n",
    "\n",
    "def plot_rewards(rewards: Sequence[float], window: int=10):\n",
    "    running_avg = np.convolve(rewards, np.ones(window), 'valid') / window\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(rewards)\n",
    "    ax.plot(window + np.arange(len(running_avg)), running_avg)\n",
    "\n",
    "def plt_animation(frames: Sequence[RenderFrame], fps: int) -> TimedAnimation:\n",
    "    \"\"\"Generate a Pyplot animation from a sequence of Gymnasium environment RGB rendering.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    imgs = []\n",
    "    if len(frames) > 0:\n",
    "        imgs.append([ax.imshow(frames[0])])\n",
    "        for a in frames[1:]:\n",
    "            imgs.append([ax.imshow(a, animated=True)])\n",
    "    # prevent showing pyplot default window\n",
    "    plt.close(fig)\n",
    "    return ArtistAnimation(fig, imgs, interval=int(1000/fps), repeat=False, blit=True)\n",
    "\n",
    "def show_video(frames: Sequence[RenderFrame], fps: int=10):\n",
    "    return display.HTML(plt_animation(frames, fps=fps).to_html5_video())\n",
    "\n",
    "def show_image(frame: RenderFrame) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creiamo l'ambiente\n",
    "\n",
    "Inizializziamo l'ambiente per gli esperimenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffw = gym.wrappers.RecordEpisodeStatistics(\n",
    "    gym.wrappers.TimeLimit(\n",
    "        gym.make(\"CliffWalking-v0\", render_mode=\"rgb_array\"),\n",
    "        max_episode_steps=200))\n",
    "\n",
    "cliffw_rec = gym.wrappers.RenderCollection(cliffw, pop_frames=False)\n",
    "\n",
    "cliffw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come \"funziona\" `CliffWalking-v0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffw.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osservazione, info = cliffw.reset()\n",
    "print(f'Osservazione: {osservazione}')\n",
    "show_image(cliffw.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo una delle azioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azione = 0\n",
    "observation, reward, _, _, _ = cliffw.step(azione)\n",
    "print(f'Azione: {azione}, Nuova osservazione: {observation}, Ricompensa: {reward}')\n",
    "show_image(cliffw.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comportamento casuale\n",
    "\n",
    "Vediamo i risultati di un comportamento totalmente casuale\n",
    "\n",
    "- ogni istante sceglie un'azione a caso\n",
    "  ```python\n",
    "  azione = env.action_space.sample()\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = run_episode(cliffw_rec)\n",
    "print(f'Punteggio finale: {info['episode']['r']}')\n",
    "show_video(cliffw_rec.render(), fps=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo di Q-Learning\n",
    "\n",
    "Vediamo una semplice implementazione dell'algoritmo di Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def q_learn(env: gym.Env, seed: Optional[int]=None,\n",
    "            episodes: int=300,\n",
    "            max_steps: int=200,\n",
    "            epsilon: float=0.1,\n",
    "            alpha: float=0.5,\n",
    "            gamma: float=0.9) -> tuple[Any, Sequence[float]]:\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards: list[float] = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        tot_reward = 0\n",
    "        for _ in range(max_steps):\n",
    "            if rng.random() > epsilon:\n",
    "                action = rng.choice(np.nonzero(np.isclose(Q[obs], np.max(Q[obs]))))[0]\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            tot_reward += reward\n",
    "            Q[obs, action] += alpha * (reward + gamma * np.max(Q[new_obs]) - Q[obs, action])\n",
    "            obs = new_obs\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        rewards.append(tot_reward)\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {tot_reward}\")\n",
    "\n",
    "    return Q, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprendimento\n",
    "\n",
    "Proviamo ad applicare l'algoritmo e vedere come si comporta l'apprendimento:\n",
    "\n",
    "- si parte con una tabella arbitraria\n",
    "- aggiorniamo la tabella per un certo numero di episodi\n",
    "- visualizziamo i risultati dei singoli episodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, rewards = q_learn(cliffw, episodes=300, max_steps=200, alpha=.5, gamma=.9, epsilon=.1)\n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valutazione\n",
    "\n",
    "Compariamo l'agente \"casuale\" con la strategia imparata\n",
    "\n",
    "- generiamo un numero di episodi\n",
    "- visualizziamo i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards([ run_episode(cliffw)[4]['episode']['r'] for _ in range(200) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards([ run_episode(cliffw, agent=lambda o: np.argmax(Q[o]))[4]['episode']['r'] for _ in range(200) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La strategia\n",
    "\n",
    "Visualizziamo la strategia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = run_episode(cliffw_rec, agent=lambda o: np.argmax(Q[o]))\n",
    "show_video(cliffw_rec.render(), fps=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo com'è fatta la Q-table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shape = cliffw.unwrapped.shape\n",
    "actions = ['⬆︎', '➡︎', '⬇︎', '⬅︎']\n",
    "\n",
    "directions = [''.join(actions[a] for a in np.nonzero(np.isclose(r, np.max(r)))[0]) for r in Q]\n",
    "\n",
    "plt.table(np.reshape(np.array(directions), map_shape), cellLoc='center', loc='center').axes.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## È veramente così semplice?\n",
    "\n",
    "Vediamo un ambiente più complicato, il [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "![lunar lander](./assets/lunar_lander.gif)\n",
    "\n",
    "Copyright © 2024 [Farama Foundation](https://farama.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complicato?\n",
    "\n",
    "- osservazione include posizione e velocità\n",
    "- valori continui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pilota automatico\n",
    "\n",
    "Vediamo l'atterraggio col pilota automatico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/user_lander.py autopilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usa una [strategia algoritmica](https://github.com/Farama-Foundation/Gymnasium/blob/d2707290b5ae8d3070f11b9f5e701d5ca8bfaa5f/gymnasium/envs/box2d/lunar_lander.py#L794):\n",
    "\n",
    "```python\n",
    "angle_targ = s[0] * 0.5 + s[2] * 1.0  # angle should point towards center\n",
    "if angle_targ > 0.4:\n",
    "    angle_targ = 0.4  # more than 0.4 radians (22 degrees) is bad\n",
    "\n",
    "# ...\n",
    "\n",
    "else:\n",
    "    a = 0\n",
    "    if hover_todo > np.abs(angle_todo) and hover_todo > 0.05:\n",
    "        a = 2\n",
    "    elif angle_todo < -0.05:\n",
    "        a = 3\n",
    "    elif angle_todo > +0.05:\n",
    "        a = 1\n",
    "return a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pilota umano\n",
    "\n",
    "Ora prova tu, i controlli sono:\n",
    "\n",
    "- barra spaziatrice per motore principale\n",
    "- freccia destra/sinistra per motori laterali\n",
    "- chiudere la finestra per uscire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/user_lander.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
