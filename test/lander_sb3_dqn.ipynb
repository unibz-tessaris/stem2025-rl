{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Stable Baseline 3 to solve Lunar Lander\n",
    "\n",
    "References:\n",
    "- [Lunar Lander - Gymnasium Documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "- [Examples â€” Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    \"LunarLander-v3\",\n",
    "    verbose=1,\n",
    "    exploration_final_eps=0.1,\n",
    "    target_update_interval=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate env for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5), progress_bar=True)\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Sequence, SupportsFloat\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import ActType, ObsType, RenderFrame\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import ArtistAnimation, TimedAnimation\n",
    "\n",
    "def plt_animation(frames: Sequence[RenderFrame], fps: int) -> TimedAnimation:\n",
    "    \"\"\"Generate a Pyplot animation from a sequence of Gymnasium environment RGB rendering.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    imgs = []\n",
    "    if len(frames) > 0:\n",
    "        imgs.append([ax.imshow(frames[0])])\n",
    "        for a in frames[1:]:\n",
    "            imgs.append([ax.imshow(a, animated=True)])\n",
    "    # prevent showing pyplot default window\n",
    "    plt.close(fig)\n",
    "    return ArtistAnimation(fig, imgs, interval=int(1000/fps), repeat=False, blit=True)\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Optional[Callable[[ObsType], ActType]]=None) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "    \"\"\"Run an episode of the environment using the provided agent (default to random), returning the last `step` method output\"\"\"\n",
    "    observation, info = env.reset()\n",
    "    if agent == None:\n",
    "        agent = lambda o: env.action_space.sample()\n",
    "    while True:\n",
    "        action = agent(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "env = gym.wrappers.RenderCollection(gym.wrappers.RecordEpisodeStatistics(gym.make(\"LunarLander-v3\", render_mode='rgb_array')), pop_frames=False)\n",
    "\n",
    "def agent_from_model(model: DQN) -> Callable[[ObsType], ActType]:\n",
    "    def __agent(obs: ObsType) -> ActType:\n",
    "        return model.predict(obs, deterministic=True)[0]\n",
    "    return __agent\n",
    "\n",
    "observation, reward, terminated, truncated, info = run_episode(env, agent_from_model(model))\n",
    "\n",
    "\n",
    "landed = terminated and reward >= 100\n",
    "total = info.get('episode', {}).get('r', None)\n",
    "print(\"{}, with a total reward of {}\".format('Landed' if landed else 'Crashed', total))\n",
    "display.HTML(plt_animation(env.render(), fps=30).to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
